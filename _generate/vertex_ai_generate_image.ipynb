{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üì∏ Product Image Generation with Vertex AI and BigQuery\n",
        "\n",
        "This notebook automates the process of identifying products in a BigQuery catalog that are missing images and using a generative AI model (Vertex AI Imagen) to create and store new product photos.\n",
        "\n",
        "**The workflow is as follows:**\n",
        "\n",
        "1.  **Query BigQuery**: Fetch a list of products where the `image_url` is missing.\n",
        "2.  **Generate Image**: For each product, create a descriptive prompt from its metadata (e.g., name, category, description) and use Vertex AI Imagen to generate a new image.\n",
        "3.  **Store in GCS**: Upload the generated image file to a specified Google Cloud Storage (GCS) bucket.\n",
        "4.  **Log to BigQuery**: Record the GCS path of the new image in a log table for tracking and integration.\n",
        "\n",
        "-----\n",
        "\n",
        "## ‚öôÔ∏è 1. Setup and Configuration\n",
        "\n",
        "First, let's install the necessary libraries and authenticate. In a Colab Enterprise environment, authentication is typically handled seamlessly."
      ],
      "metadata": {
        "id": "Ts3QwVLjrZk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required Google Cloud libraries\n",
        "!pip install --upgrade google-cloud-aiplatform google-cloud-bigquery google-cloud-storage db-dtypes\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jwF2ZZDYrZk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, configure the variables for your specific GCP environment. **You must change these values to match your project setup.**"
      ],
      "metadata": {
        "id": "dM1fAVRxrZk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After the session restart, re-import libraries and initialize clients.\n",
        "import google.colab.auth\n",
        "from google.cloud import aiplatform, bigquery, storage\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate user. In Colab Enterprise, this is often seamless.\n",
        "# google.colab.auth.authenticate_user()\n",
        "\n",
        "# --- USER CONFIGURATION ---\n",
        "\n",
        "# GCP Project Details\n",
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"            # @param {type:\"string\"}\n",
        "\n",
        "# BigQuery Details\n",
        "BQ_DATASET_ID = \"retail\"                  # @param {type:\"string\"}\n",
        "BQ_PRODUCTS_VIEW = \"view_product_wo_image\"  # @param {type:\"string\"}\n",
        "BQ_IMAGE_LOG_TABLE = \"generated_image_log\"         # @param {type:\"string\"}\n",
        "BQ_QUERY_LIMIT = 100 # @param {type:\"integer\"}\n",
        "\n",
        "# Google Cloud Storage Details\n",
        "GCS_BUCKET_NAME = \"\" # @param {type:\"string\"}\n",
        "\n",
        "# --- IMAGE GENERATION CONFIGURATION ---\n",
        "# You can adjust this number based on your quotas and desired concurrency level\n",
        "MAX_WORKERS = 8 # @param {type:\"integer\"}\n",
        "\n",
        "# See model documentation for all options: https://cloud.google.com/vertex-ai/docs/generative-ai/image/overview\n",
        "IMAGEN_MODEL_NAME = \"imagen-4.0-fast-generate-001\" # @param {type:\"string\"}\n",
        "IMAGEN_MODEL_NAME = IMAGEN_MODEL_NAME or \"imagen-4.0-generate-001\"\n",
        "NUMBER_OF_IMAGES_TO_GENERATE = 1  # Number of images to generate per product\n",
        "IMAGE_STYLE_PRESET = \"photorealistic\" # Options: \"photorealistic\", \"digital_art\", \"cinematic\", etc.\n",
        "ASPECT_RATIO = \"1:1\" # Options: \"1:1\", \"16:9\", \"9:16\", etc.\n",
        "SAFETY_FILTER_LEVEL = \"block_only_high\" # Options: \"block_most\", \"block_some\", \"block_few\". \"block_few\" is the most permissive.\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "A professional, high-resolution product photograph of a {categories},\n",
        "specifically: {description}.\n",
        "The product is centered on a clean, solid light-grey background.\n",
        "The lighting is bright and even, highlighting the product's features.\n",
        "Style: {style}.\n",
        "\"\"\"\n",
        "\n",
        "# --- INITIALIZE CLIENTS ---\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "print(f\"Configuration loaded for project: {PROJECT_ID}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "8EDFE5mOrZk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## üìä 2. Fetch Products from BigQuery\n",
        "\n",
        "This function queries your BigQuery view to get the list of products that need an image. We'll use a `LIMIT` clause to control how many products we process in one run, which is useful for testing.\n",
        "\n",
        "**Prerequisite**: You need a view in BigQuery (`products_without_images_view`) that returns at least `product_id`, `product_description`, and `product_category` for products where an image URL is not present."
      ],
      "metadata": {
        "id": "sNPVsZj7rZk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_products_without_images(limit: int = 1000) -> pd.DataFrame:\n",
        "    \"\"\"Queries BigQuery to get a list of products with no image URL.\n",
        "\n",
        "    Args:\n",
        "        limit: The maximum number of products to fetch.\n",
        "\n",
        "    Returns:\n",
        "        A pandas DataFrame containing product data.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching up to {limit} products from BigQuery...\")\n",
        "    query = f\"\"\"\n",
        "        SELECT\n",
        "            id,\n",
        "            title,\n",
        "            description,\n",
        "            categories\n",
        "        FROM\n",
        "            `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_PRODUCTS_VIEW}`\n",
        "        LIMIT {limit}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = bq_client.query(query).to_dataframe()\n",
        "        print(f\"Successfully fetched {len(df)} products.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while querying BigQuery: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Fetch the product data\n",
        "products_df = fetch_products_without_images(limit=BQ_QUERY_LIMIT)\n",
        "display(products_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "4qKXfVKXrZk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## üé® 3. Generate and Store Product Image\n",
        "\n",
        "This function takes the data for a single product, builds a descriptive prompt, calls the Vertex AI Imagen model to generate the image, and then uploads the resulting image file to your GCS bucket."
      ],
      "metadata": {
        "id": "3ddxmiQ3rZk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.vision_models import ImageGenerationModel\n",
        "import traceback\n",
        "\n",
        "def generate_and_store_image(id: str, title: str, description: str, categories: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates an image based on product data and stores it in GCS.\n",
        "\n",
        "    Args:\n",
        "        id: The unique identifier for the product.\n",
        "        title: The title or name of the product.\n",
        "        description: The description of the product.\n",
        "        categories: The product's categories.\n",
        "\n",
        "    Returns:\n",
        "        The GCS URI of the stored image (e.g., \"gs://bucket/image.png\").\n",
        "        Returns an empty string if generation or upload fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Construct the detailed prompt\n",
        "        prompt = PROMPT_TEMPLATE.format(\n",
        "            categories=categories,\n",
        "            description=description,\n",
        "            style=IMAGE_STYLE_PRESET\n",
        "        )\n",
        "        print(f\"   Prompt for product '{id}':\\n   '{prompt[:150]}...'\")\n",
        "\n",
        "        # 2. Initialize the model and generate the image\n",
        "        model = ImageGenerationModel.from_pretrained(IMAGEN_MODEL_NAME)\n",
        "        response = model.generate_images(\n",
        "            prompt=prompt,\n",
        "            number_of_images=NUMBER_OF_IMAGES_TO_GENERATE,\n",
        "            aspect_ratio=ASPECT_RATIO,\n",
        "            safety_filter_level=SAFETY_FILTER_LEVEL\n",
        "        )\n",
        "\n",
        "        # 3. **Crucial Check**: Ensure the model returned an image.\n",
        "        if not response.images:\n",
        "            print(f\"‚ö†Ô∏è Model returned no images for product '{id}'. This might be due to safety filters or a problematic prompt.\")\n",
        "            return \"\"\n",
        "\n",
        "        image_bytes = response.images[0]._image_bytes\n",
        "\n",
        "        # 4. Upload the image to Google Cloud Storage\n",
        "        bucket = storage_client.get_bucket(GCS_BUCKET_NAME)\n",
        "        blob_name = f\"product_images/{id}.png\"\n",
        "        blob = bucket.blob(blob_name)\n",
        "\n",
        "        blob.upload_from_string(image_bytes, content_type=\"image/png\")\n",
        "        gcs_uri = f\"gs://{GCS_BUCKET_NAME}/{blob_name}\"\n",
        "\n",
        "        print(f\"‚úÖ Image for product '{id}' successfully stored at: {gcs_uri}\")\n",
        "        return gcs_uri\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to generate or store image for product '{id}': {e}\")\n",
        "        print(\"   --- Full Error Traceback ---\")\n",
        "        print(traceback.format_exc())\n",
        "        print(\"   --------------------------\")\n",
        "        return \"\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lBDoiDV_rZk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## üìù 4. Log Image Path to BigQuery\n",
        "\n",
        "After a new image is successfully created and stored, this function logs the `product_id` and the new `gcs_uri` into a tracking table in BigQuery.\n",
        "\n",
        "**Prerequisite**: You need a table in BigQuery (`generated_image_log`) with at least these columns: `product_id` (STRING), `gcs_uri` (STRING), and `log_timestamp` (TIMESTAMP)."
      ],
      "metadata": {
        "id": "ILXgxpVbrZk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_image_path_to_bigquery(id: str, gcs_uri: str):\n",
        "    \"\"\"\n",
        "    Inserts a record into a BigQuery log table.\n",
        "\n",
        "    Args:\n",
        "        id: The ID of the product.\n",
        "        gcs_uri: The GCS path of the generated image.\n",
        "    \"\"\"\n",
        "    table_ref = bq_client.dataset(BQ_DATASET_ID).table(BQ_IMAGE_LOG_TABLE)\n",
        "    rows_to_insert = [\n",
        "        {\n",
        "            \"id\": id,\n",
        "            \"gcs_uri\": gcs_uri,\n",
        "            \"log_timestamp\": pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    errors = bq_client.insert_rows_json(table_ref, rows_to_insert)\n",
        "    if not errors:\n",
        "        print(f\"   Successfully logged path for product '{id}' to BigQuery.\")\n",
        "    else:\n",
        "        print(f\"   Encountered errors while inserting rows to BigQuery: {errors}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lESIIE00rZk4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6125393"
      },
      "source": [
        "-----\n",
        "\n",
        "## üöÄ 5. Enhanced Workflow for Batch Processing\n",
        "\n",
        "The sequential `for` loop in the previous step is easy to follow but inefficient for large datasets. When processing thousands of products, making API calls one by one becomes very slow.\n",
        "\n",
        "To significantly boost performance, we can parallelize the work. This enhanced workflow uses a `ThreadPoolExecutor` to run the `generate_and_store_image` function for multiple products concurrently. We will also batch the final logging step to BigQuery for greater efficiency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b1dc8de"
      },
      "source": [
        "# Import libraries for concurrency and progress tracking\n",
        "import concurrent.futures\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# @title Configure Parallelism"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26913395"
      },
      "source": [
        "def log_image_paths_to_bigquery_batch(results: list):\n",
        "    \"\"\"\n",
        "    Inserts a batch of records into a BigQuery log table.\n",
        "\n",
        "    Args:\n",
        "        results: A list of dictionaries, where each dict contains\n",
        "                 'id', 'gcs_uri', and 'log_timestamp'.\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        print(\"No new images were generated, skipping BigQuery log.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Logging {len(results)} new image paths to BigQuery in a single batch...\")\n",
        "    table_ref = bq_client.dataset(BQ_DATASET_ID).table(BQ_IMAGE_LOG_TABLE)\n",
        "\n",
        "    errors = bq_client.insert_rows_json(table_ref, results)\n",
        "    if not errors:\n",
        "        print(f\"   Successfully logged {len(results)} records to BigQuery.\")\n",
        "    else:\n",
        "        print(f\"   Encountered errors while inserting rows to BigQuery: {errors}\")\n",
        "\n",
        "def process_product(row):\n",
        "    \"\"\"\n",
        "    Wrapper function to process a single row from the DataFrame.\n",
        "    This makes it easy to use with the ThreadPoolExecutor.\n",
        "    \"\"\"\n",
        "    product_id = row['id']\n",
        "\n",
        "    gcs_path = generate_and_store_image(\n",
        "        id=product_id,\n",
        "        title=row['title'],\n",
        "        description=row['description'],\n",
        "        categories=row['categories']\n",
        "    )\n",
        "\n",
        "    if gcs_path:\n",
        "        return {\n",
        "            \"id\": product_id,\n",
        "            \"gcs_uri\": gcs_path,\n",
        "            \"log_timestamp\": pd.Timestamp.now().isoformat()\n",
        "        }\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf52ac7c"
      },
      "source": [
        "if not products_df.empty:\n",
        "    print(f\"\\nStarting the ENHANCED image generation workflow with up to {MAX_WORKERS} parallel workers...\\n\" + \"=\"*60)\n",
        "\n",
        "    successful_results = []\n",
        "    # We use a list comprehension to create a list of rows to process\n",
        "    tasks_to_run = [row for _, row in products_df.iterrows()]\n",
        "\n",
        "    # Use ThreadPoolExecutor to process products in parallel\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        # tqdm will create a visual progress bar\n",
        "        future_to_row = {executor.submit(process_product, row): row for row in tasks_to_run}\n",
        "        for future in tqdm(concurrent.futures.as_completed(future_to_row), total=len(tasks_to_run)):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                successful_results.append(result)\n",
        "\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    # After all threads are complete, log all successful results in one batch\n",
        "    log_image_paths_to_bigquery_batch(successful_results)\n",
        "\n",
        "    print(\"=\"*60 + f\"\\nEnhanced workflow finished! Generated {len(successful_results)} new images.\")\n",
        "else:\n",
        "    print(\"No products found that need images. All done!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "vertex_ai_generate_image.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}