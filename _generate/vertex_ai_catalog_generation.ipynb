{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0OdPy5BY5kB"
      },
      "source": [
        "# Vertex AI Search for Retail: Product Catalog Generation\n",
        "\n",
        "This notebook demonstrates how to use a large language model (LLM) on Vertex AI to generate a synthetic product catalog for a retailer. This is a common first step when building a demo or proof-of-concept for Vertex AI Search for Retail.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "*   **Modular Configuration:** Uses external files for schema, prompts, and other configurations, making it easy to adapt for different retailers or use cases.\n",
        "*   **Scalable Data Generation:** Leverages concurrent processing to generate a large number of products efficiently.\n",
        "*   **Robust JSON Parsing:** Includes logic to handle and clean potential formatting issues in the LLM's JSON output.\n",
        "*   **BigQuery Integration:** Creates a BigQuery dataset and table, and loads the generated catalog data.\n",
        "\n",
        "## 1. Setup and Authentication\n",
        "\n",
        "Install necessary libraries, authenticate to Google Cloud, and set up project-specific variables."
      ],
      "id": "_0OdPy5BY5kB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Install Packages",
        "id": "f3Xae-e-Y5kD"
      },
      "outputs": [],
      "source": [
        "!pip install google-cloud-aiplatform google-cloud-bigquery google-cloud-secret-manager tqdm pandas"
      ],
      "id": "f3Xae-e-Y5kD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Authenticate to Google Cloud",
        "id": "CSz0DJBCY5kD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# If you are running this notebook in a Colab environment, you will need to\n",
        "# authenticate your user account.\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()"
      ],
      "id": "CSz0DJBCY5kD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "814aea9a",
      "metadata": {
        "id": "814aea9a"
      },
      "outputs": [],
      "source": [
        "!GOOGLE_CLOUD_PROJECT=`gcloud config set project partarch-ecommerce-demo`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Define Project Configuration",
        "id": "qEkIRCsKY5kD"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Configure Project and Generation Parameters\n",
        "import re\n",
        "import os\n",
        "\n",
        "#@markdown ### Project and GCP Configuration\n",
        "#@markdown Configure the core GCP and Retailer settings here.\n",
        "PROJECT_ID = \"partarch-ecommerce-demo\" #@param {type:\"string\"}\n",
        "LOCATION = \"\" #@param {type:\"string\"}\n",
        "LOCATION = LOCATION or \"us-central1\"\n",
        "RETAILER = \"Toys R Us\" #@param {type:\"string\"}\n",
        "RETAILER = RETAILER or \"Wayfair\"\n",
        "MODEL_NAME = \"\" #@param {type:\"string\"}\n",
        "MODEL_NAME = MODEL_NAME or \"gemini-2.5-pro\"\n",
        "#@markdown ---\n",
        "#@markdown ### URIs\n",
        "#@markdown Configure the URIs for the site and images.\n",
        "SITE_URI = \"https://vibe-commerce-713683707236.us-central1.run.app/\" #@param {type:\"string\"}\n",
        "#@markdown   Example: `https://vibe-commerce-123456789000.us-central1.run.app/`\n",
        "IMAGE_URI = \"https://storage.googleapis.com/{PROJECT_ID}-images/product_images/\"\n",
        "#@markdown ---\n",
        "#@markdown ### Model Generation Parameters\n",
        "#@markdown Configure the model's generation settings.\n",
        "MAX_OUTPUT_TOKENS = 65535 #@param {type:\"integer\"}\n",
        "TEMPERATURE = 1.0 #@param {type:\"number\"}\n",
        "TOP_P = 0.95 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Data Generation Parameters\n",
        "#@markdown Adjust the volume and concurrency of the data generation process.\n",
        "NUMBER_OF_PRIMARY_PRODUCTS = 20 #@param {type:\"integer\"}\n",
        "NUMBER_OF_VARIANTS_PER_PRIMARY = 5 #@param {type:\"integer\"}\n",
        "PRODUCTS_PER_BATCH = 10 #@param {type:\"integer\"}\n",
        "MAX_WORKERS = 20 #@param {type:\"integer\"}\n",
        "#@markdown ---\n",
        "#@markdown ### Product Categories\n",
        "#@markdown Provide a path to a local file with one category per line. If left blank, a default list is used.\n",
        "#@markdown **Sample Format:**\n",
        "#@markdown ```\n",
        "#@markdown Furniture > Living Room Furniture > Sofas & Couches\n",
        "#@markdown Furniture > Living Room Furniture > Coffee Tables\n",
        "#@markdown Furniture > Bedroom Furniture > Beds\n",
        "#@markdown Furniture > Bedroom Furniture > Dressers & Chests\n",
        "#@markdown Outdoor > Outdoor Seating > Patio Sofas\n",
        "#@markdown ```\n",
        "CATEGORIES_FILE_PATH = \"custom_categories.txt\" #@param {type:\"string\"}\n",
        "\n",
        "# --- Start of Processing Logic ---\n",
        "\n",
        "# Replace placeholder in IMAGE_URI\n",
        "IMAGE_URI = IMAGE_URI.format(PROJECT_ID=PROJECT_ID)\n",
        "\n",
        "# --- Load Product Categories ---\n",
        "product_categories_content = \"\"\n",
        "default_categories = \"\"\"Furniture > Living Room Furniture > Sofas & Couches\n",
        "Furniture > Living Room Furniture > Coffee Tables\n",
        "Furniture > Bedroom Furniture > Beds\n",
        "Furniture > Bedroom Furniture > Dressers & Chests\n",
        "Outdoor > Outdoor Seating > Patio Sofas\n",
        "Outdoor > Grills & Outdoor Cooking > Gas Grills\n",
        "Bed & Bath > Bedding > Comforters & Sets\n",
        "Bed & Bath > Bath Linens > Bath Towels\n",
        "Rugs > Area Rugs > Modern Rugs\n",
        "Decor & Pillows > Wall Decor > Wall Art\n",
        "Lighting > Ceiling Fans > Fans with Lights\n",
        "Kitchen & Tabletop > Cookware > Pots & Pans Sets\n",
        "Storage & Organization > Closet Organizers > Closet Systems\n",
        "Baby & Kids > Nursery Furniture > Cribs\n",
        "Home Improvement > Flooring > Hardwood Flooring\n",
        "\"\"\"\n",
        "\n",
        "if CATEGORIES_FILE_PATH.strip():\n",
        "    path_to_check = CATEGORIES_FILE_PATH.strip()\n",
        "    # In Colab, if a user provides a filename without a full path,\n",
        "    # it's helpful to assume it's in the default /content/ directory.\n",
        "    if not os.path.isabs(path_to_check):\n",
        "        path_to_check = os.path.join(\"/content\", path_to_check)\n",
        "\n",
        "    print(f\"Attempting to load categories from file: {path_to_check}\")\n",
        "    if not os.path.exists(path_to_check):\n",
        "        raise FileNotFoundError(f\"The specified category file was not found: {path_to_check}\")\n",
        "\n",
        "    with open(path_to_check, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    validated_lines = []\n",
        "    for i, line in enumerate(lines):\n",
        "        stripped_line = line.strip()\n",
        "        if not stripped_line:\n",
        "            continue\n",
        "        # Basic validation: check for unexpected characters. Allows letters, numbers, spaces, and common delimiters.\n",
        "        if not re.match(r\"^[a-zA-Z0-9\\s>&/(),.'-]+\", stripped_line):\n",
        "             raise ValueError(f\"Invalid characters found in category file on line {i+1}: '{stripped_line}'\")\n",
        "        validated_lines.append(stripped_line)\n",
        "\n",
        "    if not validated_lines:\n",
        "        raise ValueError(f\"The category file '{path_to_check}' is empty or contains only whitespace.\")\n",
        "\n",
        "    product_categories_content = \"\\n\".join(validated_lines)\n",
        "    print(\"Successfully loaded and validated categories from file.\")\n",
        "\n",
        "else:\n",
        "    print(\"No category file provided. Using the default list.\")\n",
        "    product_categories_content = default_categories\n",
        "\n",
        "\n",
        "# The GCS bucket name will be the same as the Project ID\n",
        "GCS_BUCKET_NAME = PROJECT_ID\n",
        "\n",
        "# --- Sanitize RETAILER input ---\n",
        "original_retailer = RETAILER\n",
        "sanitized_retailer = re.sub(r'[^a-z0-9]', '', original_retailer.lower())\n",
        "\n",
        "if original_retailer != sanitized_retailer:\n",
        "    print(f\"⚠️ WARNING: The retailer name '{original_retailer}' contained invalid characters or was not lowercase.\")\n",
        "    RETAILER = sanitized_retailer\n",
        "    print(f\"   It has been sanitized to: '{RETAILER}'\\n\")\n",
        "else:\n",
        "    RETAILER = sanitized_retailer\n",
        "\n",
        "# BigQuery and GCS configuration (Derived from the above)\n",
        "BQ_DATASET = \"retail\"\n",
        "BQ_TABLE = f\"products-{RETAILER}\"\n",
        "GCS_CATALOG_DIR = f\"gs://{GCS_BUCKET_NAME}/retail_catalog/{RETAILER}\"\n",
        "GENERATED_JSONL_GCS_PATH = f\"{GCS_CATALOG_DIR}/products.jsonl\"\n",
        "\n",
        "# --- Display Configuration ---\n",
        "print(\"\\n--- Project and GCP Configuration ---\")\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"GCS Bucket: {GCS_BUCKET_NAME} (set to be the same as Project ID)\")\n",
        "print(f\"Location: {LOCATION}\")\n",
        "print(f\"Retailer: {RETAILER}\")\n",
        "print(f\"Model Name: {MODEL_NAME}\")\n",
        "print(f\"Site URI: {SITE_URI}\")\n",
        "print(f\"Image URI: {IMAGE_URI}\")\n",
        "print(\"\\n--- Model Generation Parameters ---\")\n",
        "print(f\"Max Output Tokens: {MAX_OUTPUT_TOKENS}\")\n",
        "print(f\"Temperature: {TEMPERATURE}\")\n",
        "print(f\"Top P: {TOP_P}\")\n",
        "print(\"\\n--- Data Generation Parameters ---\")\n",
        "print(f\"Primary Products to Generate: {NUMBER_OF_PRIMARY_PRODUCTS}\")\n",
        "print(f\"Variants per Primary: {NUMBER_OF_VARIANTS_PER_PRIMARY}\")\n",
        "print(f\"Total Products (approx): {NUMBER_OF_PRIMARY_PRODUCTS * (1 + NUMBER_OF_VARIANTS_PER_PRIMARY)}\")\n",
        "print(f\"Products per Batch Call: {PRODUCTS_PER_BATCH}\")\n",
        "print(f\"Max Parallel Workers: {MAX_WORKERS}\")\n",
        "print(\"\\n--- Product Categories to be Used ---\")\n",
        "print(product_categories_content)\n",
        "print(\"\\n--- Derived Configuration ---\")\n",
        "print(f\"BigQuery Destination: {PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\")\n",
        "print(f\"GCS Path: {GENERATED_JSONL_GCS_PATH}\")\n"
      ],
      "id": "qEkIRCsKY5kD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Initialize Vertex AI and BigQuery Clients",
        "id": "1sUekeEMY5kE"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "from google.cloud import bigquery\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "bq_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "print(\"Vertex AI and BigQuery clients initialized.\")"
      ],
      "id": "1sUekeEMY5kE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY4LDpAHY5kE"
      },
      "source": [
        "## 2. Load Configurations\n",
        "\n",
        "Load the schema, field requirements, product categories, and the generation prompt from external files. This makes the notebook highly reusable."
      ],
      "id": "NY4LDpAHY5kE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Create Configuration Directories and Files",
        "id": "OePVopXjY5kE"
      },
      "outputs": [],
      "source": [
        "# This step creates the necessary configuration files in a human-readable format.\n",
        "# In a real-world scenario, you would upload these files instead of creating them here.\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs('config', exist_ok=True)\n",
        "os.makedirs('prompts', exist_ok=True)\n",
        "\n",
        "# 1. Create schema.json in a readable way\n",
        "schema_data = [\n",
        "    { \"name\": \"name\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"id\", \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "    { \"name\": \"type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"primaryProductId\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"collectionMemberIds\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"gtin\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"categories\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"title\", \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "    { \"name\": \"brands\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"description\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"languageCode\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"attributes\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"fields\": [\n",
        "        { \"name\": \"key\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"value\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\", \"fields\": [\n",
        "            { \"name\": \"text\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "            { \"name\": \"numbers\", \"type\": \"FLOAT\", \"mode\": \"REPEATED\" }\n",
        "        ] }\n",
        "    ] },\n",
        "    { \"name\": \"tags\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"priceInfo\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\", \"fields\": [\n",
        "        { \"name\": \"currencyCode\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"price\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"originalPrice\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"cost\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"priceEffectiveTime\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"priceExpireTime\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }\n",
        "    ] },\n",
        "    { \"name\": \"rating\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\", \"fields\": [\n",
        "        { \"name\": \"ratingCount\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"averageRating\", \"type\": \"FLOAT\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"ratingHistogram\", \"type\": \"INTEGER\", \"mode\": \"REPEATED\" }\n",
        "    ] },\n",
        "    { \"name\": \"expireTime\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"ttl\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\", \"fields\": [\n",
        "        { \"name\": \"seconds\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"nanos\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\" }\n",
        "    ] },\n",
        "    { \"name\": \"availableTime\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"availability\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"availableQuantity\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"fulfillmentInfo\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"fields\": [\n",
        "        { \"name\": \"type\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"placeIds\", \"type\": \"STRING\", \"mode\": \"REPEATED\" }\n",
        "    ] },\n",
        "    { \"name\": \"uri\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"images\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"fields\": [\n",
        "        { \"name\": \"uri\", \"type\": \"STRING\", \"mode\": \"REQUIRED\" },\n",
        "        { \"name\": \"height\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\" },\n",
        "        { \"name\": \"width\", \"type\": \"INTEGER\", \"mode\": \"NULLABLE\" }\n",
        "    ] },\n",
        "    { \"name\": \"audience\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\", \"fields\": [\n",
        "        { \"name\": \"genders\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "        { \"name\": \"ageGroups\", \"type\": \"STRING\", \"mode\": \"REPEATED\" }\n",
        "    ] },\n",
        "    { \"name\": \"colorInfo\", \"type\": \"RECORD\", \"mode\": \"NULLABLE\", \"fields\": [\n",
        "        { \"name\": \"colorFamilies\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "        { \"name\": \"colors\", \"type\": \"STRING\", \"mode\": \"REPEATED\" }\n",
        "    ] },\n",
        "    { \"name\": \"sizes\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"materials\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"patterns\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"conditions\", \"type\": \"STRING\", \"mode\": \"REPEATED\" },\n",
        "    { \"name\": \"publishTime\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" },\n",
        "    { \"name\": \"promotions\", \"type\": \"RECORD\", \"mode\": \"REPEATED\", \"fields\": [\n",
        "        { \"name\": \"promotionId\", \"type\": \"STRING\", \"mode\": \"NULLABLE\" }\n",
        "    ] }\n",
        "]\n",
        "with open('config/schema.json', 'w') as f:\n",
        "    json.dump(schema_data, f, indent=2)\n",
        "\n",
        "# 2. Create field_requirements.txt in a readable way\n",
        "field_requirements_content = \"\"\"\n",
        "name: Immutable. Full resource name of the product, such as projects/*/locations/global/catalogs/default_catalog/branches/default_branch/products/productId.\n",
        "id: Immutable. Product identifier, which is the final component of name. For example, this field is \"id_1\", if name is projects/*/locations/global/catalogs/default_catalog/branches/default_branch/products/id_1. CRITICAL: THIS ID MUST BE GLOBALLY UNIQUE AND IS CASE-INSENSITIVE. DO NOT REPEAT ANY IDS. This field must be a UTF-8 encoded string with a length limit of 128 characters.\n",
        "type: Immutable. The type of the product. Must be one of 'PRIMARY', 'VARIANT', or 'COLLECTION'.\n",
        "primaryProductId: Variant group identifier. Must be an id of another product. For PRIMARY products, this field can only be empty or set to the same value as id. For VARIANT products, this field cannot be empty.\n",
        "collectionMemberIds: IMPORTANT: This field should ONLY be populated when `type` is 'COLLECTION'. For 'PRIMARY' and 'VARIANT' products, this field must be omitted or be an empty array.\n",
        "gtin: The Global Trade Item Number (GTIN) of the product. Must be a valid, numerical GTIN (e.g., a 12 or 13-digit UPC/EAN). Do NOT include any letters, spaces, or symbols.\n",
        "categories: Product categories. Use '>' to separate hierarchies. Must be set for PRIMARY products. At most 250 values are allowed. Each value must be a UTF-8 encoded string with a length limit of 5,000 characters.\n",
        "title: Required. Product title. Must be a UTF-8 encoded string with a length limit of 1,000 characters.\n",
        "brands: The brands of the product. A maximum of 30 brands are allowed. Each brand must be a UTF-8 encoded string with a length limit of 1,000 characters.\n",
        "description: Product description. Must be a UTF-8 encoded string with a length limit of 5,000 characters.\n",
        "languageCode: Language of the title/description. Use BCP 47 language tags. Defaults to \"en-US\".\n",
        "attributes: Highly encouraged. Extra product attributes. Max 200 entries. Key must match pattern: [a-zA-Z0-9][a-zA-Z0-9_]*. CRITICAL: For each attribute object, you must provide a value for EITHER `text` OR `numbers`, but NEVER BOTH. Do not set both `text` and `numbers` fields for the same attribute.\n",
        "tags: Custom tags for filtering. At most 250 values are allowed. Each value must be a UTF-8 encoded string with a length limit of 1,000 characters.\n",
        "priceInfo: Product price and cost information.\n",
        "priceInfo.currencyCode: The 3-letter currency code defined in ISO 4217.\n",
        "priceInfo.price: Price of the product.\n",
        "priceInfo.originalPrice: Price of the product without any discount. Should be >= price.\n",
        "priceInfo.cost: The costs associated with the sale of a particular product.\n",
        "priceInfo.priceEffectiveTime: Timestamp (RFC 3339) when the price starts to be effective. CRITICAL: If you set this field, you MUST also set `originalPrice`.\n",
        "priceInfo.priceExpireTime: Timestamp (RFC 3339) when the price stops to be effective.\n",
        "rating: The rating of this product.\n",
        "rating.ratingCount: The total number of ratings. Must be non-negative.\n",
        "rating.averageRating: The average rating of the Product, scaled at 1-5.\n",
        "rating.ratingHistogram: List of rating counts per rating value (index = rating - 1). Size must be 5 if non-empty.\n",
        "expireTime: Timestamp (RFC 3339) when the product expires. Must be later than availableTime and publishTime. CRITICAL: This field and `ttl` are mutually exclusive. You can only set one of them for a product, not both.\n",
        "ttl: Input only. The TTL (time to live) of the product. CRITICAL: This field and `expireTime` are mutually exclusive. You can only set one of them for a product, not both. The `seconds` value within this field MUST be a non-negative integer. DO NOT provide negative values for `ttl.seconds`.\n",
        "availableTime: The timestamp (RFC 3339) when this Product becomes available for Search.\n",
        "availability: The online availability of the Product. One of 'IN_STOCK', 'OUT_OF_STOCK', 'PREORDER', 'BACKORDER'. Default to 'IN_STOCK'.\n",
        "availableQuantity: The available quantity of the item.\n",
        "fulfillmentInfo: Fulfillment information.\n",
        "fulfillmentInfo.type: The fulfillment type. Must be one of 'custom-type-1', 'custom-type-2', 'custom-type-3', 'custom-type-4', 'custom-type-5', 'next-day-delivery', 'pickup-in-store', 'same-day-delivery', 'ship-to-store'.\n",
        "fulfillmentInfo.placeIds: The IDs for this type, such as store IDs. IMPORTANT: Each ID must match the pattern [a-zA-Z0-9_]+ (letters, numbers, and underscores only, NO hyphens). For example: \"store_123\", \"warehouse_a\". Max 3000 values.\n",
        "uri: Canonical URL directly linking to the product detail page. Length limit of 5,000 characters.\n",
        "images: Product images. Main image first. A maximum of 300 images are allowed.\n",
        "images.uri: Required. URI of the image. Length limit of 5,000 characters.\n",
        "images.height: Height of the image in pixels. Must be non-negative.\n",
        "images.width: Width of the image in pixels. Must be non-negative.\n",
        "audience: The target group associated with a given audience.\n",
        "audience.genders: Genders of the audience. e.g., \"male\", \"female\", \"unisex\". At most 5 values.\n",
        "audience.ageGroups: Age groups of the audience. e.g., \"newborn\", \"infant\", \"toddler\", \"kids\", \"adult\". At most 5 values.\n",
        "colorInfo: The color of the product.\n",
        "colorInfo.colorFamilies: Standard color families. e.g., \"Red\", \"Blue\", \"Green\". Max 5 values.\n",
        "colorInfo.colors: The color display names. Max 75 colors.\n",
        "sizes: The size of the product. e.g., \"S\", \"M\", \"L\". Max 20 values.\n",
        "materials: The material of the product. e.g., \"leather\", \"wooden\". Max 20 values.\n",
        "patterns: The pattern or graphic print of the product. e.g., \"striped\", \"polka dot\". Max 20 values.\n",
        "conditions: The condition of the product. e.g., \"new\", \"refurbished\", \"used\". Max 1 value.\n",
        "publishTime: The timestamp (RFC 3339) when the product is published by the retailer for the first time.\n",
        "promotions: The promotions applied to the product. Max 10 values.\n",
        "promotions.promotionId: Promotion identifier.\n",
        "\"\"\"\n",
        "with open('config/field_requirements.txt', 'w') as f:\n",
        "    f.write(field_requirements_content.strip())\n",
        "\n",
        "# 3. Create product_categories.txt from the configured variable\n",
        "# The 'product_categories_content' variable is now defined in the main configuration cell.\n",
        "with open('config/product_categories.txt', 'w') as f:\n",
        "    f.write(product_categories_content.strip())\n",
        "\n",
        "# 4. Create data_generation_prompt_primary.txt\n",
        "data_generation_prompt_primary_content = \"\"\"\n",
        "You are an expert in generating synthetic data for retail product catalogs.\n",
        "Your task is to generate a list of {num_products} unique and realistic product entries for the retailer '{retailer}'.\n",
        "\n",
        "**ABSOLUTELY CRITICAL RULES:**\n",
        "1.  **GLOBALLY UNIQUE ID IS PARAMOUNT:** Every single product you generate in the output JSON array **MUST** have a completely unique `id`. This ID must be globally unique across all products ever generated and is case-insensitive. **DO NOT REPEAT OR REUSE ANY IDs, EVER.** This is the most important rule.\n",
        "2.  All products you generate MUST have their `type` field set to 'PRIMARY'.\n",
        "\n",
        "The products should belong to the following categories:\n",
        "--- START PRODUCT CATEGORIES ---\n",
        "{categories}\n",
        "--- END PRODUCT CATEGORIES ---\n",
        "\n",
        "Each product entry must strictly adhere to the following JSON schema. Do not add any fields that are not in the schema.\n",
        "--- START SCHEMA ---\n",
        "{schema}\n",
        "--- END SCHEMA ---\n",
        "\n",
        "Pay close attention to the following requirements and constraints for each field:\n",
        "--- START FIELD REQUIREMENTS ---\n",
        "{requirements}\n",
        "--- END FIELD REQUIREMENTS ---\n",
        "\n",
        "IMPORTANT: The entire response must be a single, valid JSON array containing the product objects.\n",
        "Do not include any text, explanations, or markdown formatting before or after the JSON array.\n",
        "The output should start with `[` and end with `]`.\n",
        "\"\"\"\n",
        "with open('prompts/data_generation_prompt_primary.txt', 'w') as f:\n",
        "    f.write(data_generation_prompt_primary_content.strip())\n",
        "\n",
        "# 5. Create data_generation_prompt_variant.txt\n",
        "data_generation_prompt_variant_content = \"\"\"\n",
        "You are an expert in generating synthetic data for retail product catalogs. Your task is to generate a list of {num_products} unique and realistic product **variants** for the retailer '{retailer}'.\n",
        "\n",
        "**ABSOLUTELY CRITICAL RULES:**\n",
        "1.  **GLOBALLY UNIQUE ID IS PARAMOUNT:** Every single product you generate in the output JSON array **MUST** have a completely unique `id`. This ID must be globally unique, case-insensitive, and different from any other primary or variant ID. **DO NOT REPEAT OR REUSE ANY IDs, EVER.** This is the most important rule.\n",
        "2.  **Every single product** you generate in the output JSON array **MUST** have its `type` field set to `'VARIANT'`. There can be no exceptions.\n",
        "3.  **Every single VARIANT product MUST** have a `primaryProductId` field.\n",
        "4.  The value for the `primaryProductId` field **MUST BE CHOSEN *EXACTLY*** from the following list of available Primary Product IDs. Do not invent, alter, or imagine any other IDs.\n",
        "\n",
        "--- START ALLOWED PRIMARY PRODUCT IDS ---\n",
        "{primary_product_ids}\n",
        "--- END ALLOWED PRIMARY PRODUCT IDS ---\n",
        "\n",
        "All other product details should be appropriate for a variant of the chosen primary product. For example, variants often differ by attributes like 'color', 'size', or 'material'.\n",
        "\n",
        "Each product entry must strictly adhere to the following JSON schema:\n",
        "--- START SCHEMA ---\n",
        "{schema}\n",
        "--- END SCHEMA ---\n",
        "\n",
        "Pay close attention to all other field requirements:\n",
        "--- START FIELD REQUIREMENTS ---\n",
        "{requirements}\n",
        "--- END FIELD REQUIREMENTS ---\n",
        "\n",
        "IMPORTANT FINAL REMINDER:\n",
        "- The entire response must be a single, valid JSON array.\n",
        "- Every object in the array must be a product of `type: 'VARIANT'`.\n",
        "- Every product must have a unique `id`.\n",
        "- Every product must have a `primaryProductId` key with a value selected from the list provided above.\n",
        "- Start the output with `[` and end with `]`. Do not include any other text.\n",
        "\"\"\"\n",
        "with open('prompts/data_generation_prompt_variant.txt', 'w') as f:\n",
        "    f.write(data_generation_prompt_variant_content.strip())\n",
        "\n",
        "print(\"Configuration files created.\")"
      ],
      "id": "OePVopXjY5kE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Load Configuration Files",
        "id": "XBQIIwgzY5kE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def load_text_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return f.read()\n",
        "\n",
        "def load_json_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Load schema and requirements\n",
        "schema_path = 'config/schema.json'\n",
        "requirements_path = 'config/field_requirements.txt'\n",
        "categories_path = 'config/product_categories.txt'\n",
        "prompt_template_path = 'prompts/data_generation_prompt.txt'\n",
        "\n",
        "bq_schema = load_json_file(schema_path)\n",
        "schema_str = json.dumps(bq_schema)\n",
        "field_requirements = load_text_file(requirements_path)\n",
        "product_categories = load_text_file(categories_path)\n",
        "prompt_template = load_text_file(prompt_template_path)\n",
        "\n",
        "print(\"Configurations loaded successfully.\")\n",
        "print(f\"Loaded {len(bq_schema)} fields in schema.\")"
      ],
      "id": "XBQIIwgzY5kE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiONZycSY5kE"
      },
      "source": [
        "### 3a. Generate PRIMARY Products\n",
        "\n",
        "This cell runs the first stage of the generation process. It uses the `data_generation_prompt_primary.txt` to create the specified `NUMBER_OF_PRODUCTS` with the type 'PRIMARY'. The unique IDs of these products are collected to be used as `primaryProductId` for the variants in the next step."
      ],
      "id": "WiONZycSY5kE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Define Generation and Parsing Functions",
        "id": "iK8um2RgY5kE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import concurrent.futures\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "from vertexai.generative_models import GenerativeModel, HarmCategory, HarmBlockThreshold\n",
        "\n",
        "def clean_and_parse_json(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Cleans the raw text output from the LLM and parses it into a Python list of dicts.\n",
        "    \"\"\"\n",
        "    start_index = text.find('[')\n",
        "    end_index = text.rfind(']')\n",
        "    if start_index == -1 or end_index == -1:\n",
        "        start_index = text.find('{')\n",
        "        end_index = text.rfind('}')\n",
        "        if start_index == -1 or end_index == -1:\n",
        "            raise ValueError(\"No JSON array or object found in the model's response.\")\n",
        "        json_str = text[start_index:end_index + 1]\n",
        "        try:\n",
        "            return [json.loads(json_str)]\n",
        "        except json.JSONDecodeError as e:\n",
        "            raise ValueError(f\"Error decoding single JSON object: {e}. String: {json_str[:500]}\")\n",
        "    json_str = text[start_index:end_index + 1]\n",
        "    try:\n",
        "        return json.loads(json_str)\n",
        "    except json.JSONDecodeError as e:\n",
        "        raise ValueError(f\"Error decoding JSON array: {e}. String: {json_str[:500]}\")\n",
        "\n",
        "def generate_product_batch_raw(prompt: str) -> list:\n",
        "    \"\"\"\n",
        "    Generates a single batch of products, returns raw parsed JSON without validation.\n",
        "    \"\"\"\n",
        "    model = GenerativeModel(MODEL_NAME)\n",
        "    safety_settings = {\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        "    generation_config = {\n",
        "        \"max_output_tokens\": MAX_OUTPUT_TOKENS,\n",
        "        \"temperature\": TEMPERATURE,\n",
        "        \"top_p\": TOP_P,\n",
        "        \"response_mime_type\": \"application/json\",\n",
        "    }\n",
        "    max_retries = 3\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)\n",
        "            if response.candidates[0].finish_reason.name == \"MAX_TOKENS\":\n",
        "                print(\"Warning: Response was truncated (MAX_TOKENS). Consider reducing PRODUCTS_PER_BATCH.\")\n",
        "                return []\n",
        "            return clean_and_parse_json(response.text)\n",
        "        except Exception as e:\n",
        "            print(f\"API call failed on attempt {attempt + 1}: {e}\")\n",
        "            if attempt == max_retries - 1: return []\n",
        "            time.sleep(2 ** attempt)\n",
        "    return []\n",
        "\n",
        "def validate_and_filter_products(products: list, existing_ids: set):\n",
        "    \"\"\"\n",
        "    Validates products, filters for unique IDs, and post-processes URIs.\n",
        "    \"\"\"\n",
        "    valid_products = []\n",
        "    newly_added_ids = set()\n",
        "    for product in products:\n",
        "        is_valid = True\n",
        "        product_id = product.get('id')\n",
        "\n",
        "        if not product_id:\n",
        "            print(f\"Warning: Discarding product with no ID: {str(product)[:100]}...\")\n",
        "            is_valid = False\n",
        "\n",
        "        elif product_id.lower() in existing_ids or product_id.lower() in newly_added_ids:\n",
        "            print(f\"Warning: Discarding duplicate product with ID: {product_id}\")\n",
        "            is_valid = False\n",
        "\n",
        "        if 'attributes' in product and product['attributes'] is not None:\n",
        "            for attr in product['attributes']:\n",
        "                if 'value' in attr and attr['value'] is not None:\n",
        "                    value_dict = attr['value']\n",
        "                    has_text = 'text' in value_dict and value_dict.get('text')\n",
        "                    has_numbers = 'numbers' in value_dict and value_dict.get('numbers')\n",
        "                    if has_text and has_numbers:\n",
        "                        del value_dict['numbers']\n",
        "\n",
        "        if 'ttl' in product and 'expireTime' in product:\n",
        "            del product['ttl']\n",
        "\n",
        "        if 'ttl' in product and isinstance(product.get('ttl'), dict):\n",
        "            ttl_seconds = product['ttl'].get('seconds')\n",
        "            if isinstance(ttl_seconds, (int, float)) and ttl_seconds < 0:\n",
        "                print(f\"Warning: Discarding product with negative ttl.seconds: {product_id}\")\n",
        "                is_valid = False\n",
        "\n",
        "        if 'priceInfo' in product and isinstance(product.get('priceInfo'), dict):\n",
        "            price_info = product['priceInfo']\n",
        "            price = price_info.get('price')\n",
        "            original_price = price_info.get('originalPrice')\n",
        "            if isinstance(price, (int, float)) and isinstance(original_price, (int, float)) and price > original_price:\n",
        "                price_info['originalPrice'] = price\n",
        "\n",
        "        if product.get('type') == 'VARIANT' and not product.get('primaryProductId'):\n",
        "             print(f\"Warning: Discarding VARIANT product with no primaryProductId: {product_id}\")\n",
        "             is_valid = False\n",
        "\n",
        "        if is_valid:\n",
        "            # --- Post-processing for URI and Image URI ---\n",
        "            if SITE_URI.strip() and product_id:\n",
        "                product['uri'] = f\"{SITE_URI.rstrip('/')}/product/{product_id}\"\n",
        "\n",
        "            if IMAGE_URI.strip() and product_id and product.get('images'):\n",
        "                for image_info in product['images']:\n",
        "                    image_filename = f\"{product_id}.png\"\n",
        "                    image_info['uri'] = f\"{IMAGE_URI.rstrip('/')}/{image_filename}\"\n",
        "                    image_info['height'] = 0\n",
        "                    image_info['width'] = 0\n",
        "\n",
        "            valid_products.append(product)\n",
        "            newly_added_ids.add(product_id.lower())\n",
        "\n",
        "    return valid_products, newly_added_ids\n",
        "\n",
        "\n",
        "# --- Main Generation Logic ---\n",
        "ALL_GENERATED_IDS = set()\n",
        "primary_prompt_template = load_text_file('prompts/data_generation_prompt_primary.txt')\n",
        "\n",
        "# --- 3a. Generate PRIMARY Products ---\n",
        "number_of_batches = math.ceil(NUMBER_OF_PRIMARY_PRODUCTS / PRODUCTS_PER_BATCH)\n",
        "print(f\"--- Generating {NUMBER_OF_PRIMARY_PRODUCTS} PRIMARY Products in {number_of_batches} batches ---\")\n",
        "\n",
        "all_primary_products = []\n",
        "primary_product_ids = []\n",
        "primary_jsonl_path = \"products_primary.jsonl\"\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "    with open(primary_jsonl_path, \"w\") as f:\n",
        "        futures = []\n",
        "        for i in range(number_of_batches):\n",
        "            prompt = primary_prompt_template.format(\n",
        "                num_products=PRODUCTS_PER_BATCH,\n",
        "                retailer=RETAILER, categories=product_categories, schema=schema_str, requirements=field_requirements,\n",
        "                existing_ids='\\n'.join(list(ALL_GENERATED_IDS))\n",
        "            )\n",
        "            futures.append(executor.submit(generate_product_batch_raw, prompt))\n",
        "\n",
        "        pbar = tqdm(concurrent.futures.as_completed(futures), total=number_of_batches, desc=\"Generating PRIMARY Batches\")\n",
        "        for future in pbar:\n",
        "            try:\n",
        "                raw_batch = future.result()\n",
        "                if raw_batch:\n",
        "                    valid_batch, new_ids = validate_and_filter_products(raw_batch, ALL_GENERATED_IDS)\n",
        "                    if valid_batch:\n",
        "                        all_primary_products.extend(valid_batch)\n",
        "                        ALL_GENERATED_IDS.update(new_ids)\n",
        "                        for product in valid_batch:\n",
        "                            f.write(json.dumps(product) + '\\n')\n",
        "                            if product.get('type') == 'PRIMARY':\n",
        "                                primary_product_ids.append(product['id'])\n",
        "                        pbar.set_postfix_str(f\"{len(valid_batch)} unique products added. Total IDs: {len(ALL_GENERATED_IDS)}\")\n",
        "            except Exception as exc:\n",
        "                pbar.set_postfix_str(f\"Batch generated an exception: {exc}\")\n",
        "\n",
        "print(\"--- PRIMARY Generation Complete ---\")\n",
        "print(f\"Total PRIMARY products generated: {len(all_primary_products)}\")"
      ],
      "id": "iK8um2RgY5kE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3a-2. Final Verification of Primary Product IDs\n",
        "\n",
        "This cell performs a final verification step to ensure that every `id` in the generated set of primary products is globally unique. It checks the `all_primary_products` list created in the previous step, reports any duplicates it finds, and removes them to prevent issues in downstream processes like variant generation or BigQuery loading."
      ],
      "metadata": {
        "id": "vZ0E4uLFMYyQ"
      },
      "id": "vZ0E4uLFMYyQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Final Verification and Cleanup for Primary Product IDs ---\n",
        "print(\"\\n--- Verifying Global Uniqueness of Primary Product IDs ---\")\n",
        "\n",
        "# The list `all_primary_products` contains all products generated in the previous step.\n",
        "# We will now perform a final check to discard any duplicates that might have been generated\n",
        "# across different parallel batches.\n",
        "\n",
        "initial_count = len(all_primary_products)\n",
        "seen_ids = set()\n",
        "unique_products = []\n",
        "\n",
        "for product in all_primary_products:\n",
        "    product_id = product.get('id')\n",
        "    if product_id:\n",
        "        # Check against a case-insensitive set of seen IDs\n",
        "        if product_id.lower() not in seen_ids:\n",
        "            unique_products.append(product)\n",
        "            seen_ids.add(product_id.lower())\n",
        "\n",
        "final_count = len(unique_products)\n",
        "duplicates_found = initial_count - final_count\n",
        "\n",
        "if duplicates_found > 0:\n",
        "    print(f\"🚨 WARNING: Found and removed {duplicates_found} duplicate product ID(s).\")\n",
        "    # Overwrite the list to contain only unique products\n",
        "    all_primary_products = unique_products\n",
        "    # It's also critical to update the list of IDs that will be used for variant generation\n",
        "    primary_product_ids = [p['id'] for p in all_primary_products if p.get('type') == 'PRIMARY']\n",
        "    print(f\"Corrected number of unique primary products: {final_count}\")\n",
        "else:\n",
        "    print(\"✅ All primary product IDs are unique. No duplicates found.\")\n",
        "\n",
        "# This ensures the list passed to the next step (variant generation) is clean."
      ],
      "metadata": {
        "id": "Ax7JIc3PMXw1"
      },
      "id": "Ax7JIc3PMXw1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3b. Generate VARIANT Products\n",
        "\n",
        "This cell runs the second stage of the generation process. It is configured to generate 10 times the number of primary products as variants.\n",
        "\n",
        "It uses the `data_generation_prompt_variant.txt` template, which is dynamically populated with a random sample of the `primary_product_ids` collected in the previous step. This ensures that the generated variants are realistically associated with existing primary products."
      ],
      "metadata": {
        "id": "hwhH_tFb7pGl"
      },
      "id": "hwhH_tFb7pGl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Run Data Generation",
        "id": "0KADCSFpY5kF"
      },
      "outputs": [],
      "source": [
        "# --- 3b. Generate VARIANT Products ---\n",
        "import random\n",
        "NUMBER_OF_VARIANTS = NUMBER_OF_PRIMARY_PRODUCTS * NUMBER_OF_VARIANTS_PER_PRIMARY\n",
        "variant_prompt_template = load_text_file('prompts/data_generation_prompt_variant.txt')\n",
        "\n",
        "if not primary_product_ids:\n",
        "    print(\"Skipping VARIANT generation: No primary product IDs were created.\")\n",
        "else:\n",
        "    number_of_variant_batches = math.ceil(NUMBER_OF_VARIANTS / PRODUCTS_PER_BATCH)\n",
        "    print(f\"--- Generating {NUMBER_OF_VARIANTS} VARIANT Products in {number_of_variant_batches} batches ---\")\n",
        "\n",
        "    all_variant_products = []\n",
        "    variant_jsonl_path = \"products_variant.jsonl\"\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        with open(variant_jsonl_path, \"w\") as f:\n",
        "            futures = []\n",
        "            for i in range(number_of_variant_batches):\n",
        "                id_sample = random.sample(primary_product_ids, k=min(50, len(primary_product_ids)))\n",
        "                variant_prompt = variant_prompt_template.format(\n",
        "                    num_products=PRODUCTS_PER_BATCH,\n",
        "                    retailer=RETAILER,\n",
        "                    categories=product_categories,\n",
        "                    schema=schema_str,\n",
        "                    requirements=field_requirements,\n",
        "                    primary_product_ids='\\n'.join(id_sample),\n",
        "                    existing_ids='\\n'.join(list(ALL_GENERATED_IDS)) # Best-effort instruction\n",
        "                )\n",
        "                futures.append(executor.submit(generate_product_batch_raw, variant_prompt))\n",
        "\n",
        "            pbar = tqdm(concurrent.futures.as_completed(futures), total=number_of_variant_batches, desc=\"Generating VARIANT Batches\")\n",
        "\n",
        "            for future in pbar:\n",
        "                try:\n",
        "                    raw_batch = future.result()\n",
        "                    if raw_batch:\n",
        "                        valid_batch, new_ids = validate_and_filter_products(raw_batch, ALL_GENERATED_IDS)\n",
        "                        if valid_batch:\n",
        "                            all_variant_products.extend(valid_batch)\n",
        "                            ALL_GENERATED_IDS.update(new_ids)\n",
        "                            for product in valid_batch:\n",
        "                                f.write(json.dumps(product) + '\\n')\n",
        "                            pbar.set_postfix_str(f\"{len(valid_batch)} unique products added. Total IDs: {len(ALL_GENERATED_IDS)}\")\n",
        "                except Exception as exc:\n",
        "                    pbar.set_postfix_str(f\"A batch generated an exception: {exc}\")\n",
        "\n",
        "    print(\"--- VARIANT Generation Complete ---\")\n",
        "    print(f\"Total VARIANT products generated: {len(all_variant_products)}\")"
      ],
      "id": "0KADCSFpY5kF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3c. Combine PRIMARY and VARIANT Products\n",
        "\n",
        "Now that both the primary and variant products have been generated into separate files, this step combines them into a single `products.jsonl` file. This consolidated file will then be used for uploading to Google Cloud Storage and loading into BigQuery."
      ],
      "metadata": {
        "id": "i2P4pkXy6LZ-"
      },
      "id": "i2P4pkXy6LZ-"
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# List of files to combine\n",
        "files_to_combine = ['products_primary.jsonl', 'products_variant.jsonl']\n",
        "combined_jsonl_path = 'products.jsonl'\n",
        "\n",
        "# The original `local_jsonl_path` variable already points to 'products.jsonl',\n",
        "# so the existing upload and BigQuery load cells will work correctly with this combined file.\n",
        "\n",
        "total_lines_written = 0\n",
        "\n",
        "with open(combined_jsonl_path, 'w') as outfile:\n",
        "    for filename in files_to_combine:\n",
        "        try:\n",
        "            with open(filename, 'r') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)\n",
        "                    total_lines_written += 1\n",
        "            print(f\"Successfully combined '{filename}'.\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: File '{filename}' not found. Skipping.\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Combined file created at: {combined_jsonl_path}\")\n",
        "print(f\"Total products in combined file: {total_lines_written}\")"
      ],
      "metadata": {
        "id": "5u2FQnUC6DWU"
      },
      "id": "5u2FQnUC6DWU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lollH9EwY5kF"
      },
      "source": [
        "## 4. Upload to Google Cloud Storage\n",
        "\n",
        "The generated JSONL file is uploaded to a GCS bucket to be used as a source for the BigQuery load job."
      ],
      "id": "lollH9EwY5kF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Upload to GCS",
        "id": "CqW_npakY5kF"
      },
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "\n",
        "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "# The destination path in GCS, removing the 'gs://' prefix\n",
        "destination_blob_name = GENERATED_JSONL_GCS_PATH.replace(f\"gs://{GCS_BUCKET_NAME}/\", \"\")\n",
        "\n",
        "# Use the correct variable from the previous file-combination cell\n",
        "upload_to_gcs(GCS_BUCKET_NAME, combined_jsonl_path, destination_blob_name)"
      ],
      "id": "CqW_npakY5kF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgBk-oGyY5kF"
      },
      "source": [
        "## 5. Load Data into BigQuery\n",
        "\n",
        "This section creates the dataset and table in BigQuery (if they don't already exist) and then loads the data from the GCS file."
      ],
      "id": "qgBk-oGyY5kF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Create Dataset and Table, then Load Data",
        "id": "IB0dyMocY5kF"
      },
      "outputs": [],
      "source": [
        "from google.cloud import bigquery\n",
        "import google.api_core.exceptions\n",
        "\n",
        "# 1. Create the BigQuery Dataset if it doesn't exist\n",
        "dataset_id = f\"{PROJECT_ID}.{BQ_DATASET}\"\n",
        "dataset = bigquery.Dataset(dataset_id)\n",
        "dataset.location = LOCATION\n",
        "try:\n",
        "    dataset = bq_client.create_dataset(dataset, timeout=30)\n",
        "    print(f\"Created dataset {dataset_id}\")\n",
        "except google.api_core.exceptions.Conflict:\n",
        "    print(f\"Dataset {dataset_id} already exists.\")\n",
        "\n",
        "# 2. Create the BigQuery Table with the specified schema\n",
        "table_id = f\"{PROJECT_ID}.{BQ_DATASET}.{BQ_TABLE}\"\n",
        "schema = [bigquery.SchemaField.from_api_repr(field) for field in bq_schema]\n",
        "table = bigquery.Table(table_id, schema=schema)\n",
        "try:\n",
        "    table = bq_client.create_table(table)\n",
        "    print(f\"Created table {table.project}.{table.dataset_id}.{table.table_id}\")\n",
        "except google.api_core.exceptions.Conflict:\n",
        "    print(f\"Table {table.project}.{table.dataset_id}.{table.table_id} already exists.\")\n",
        "\n",
        "# 3. Load the data from GCS into the BigQuery table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        "    schema=schema,\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE, # Overwrite table if it exists\n",
        ")\n",
        "\n",
        "load_job = bq_client.load_table_from_uri(\n",
        "    GENERATED_JSONL_GCS_PATH, table_id, job_config=job_config\n",
        ")\n",
        "\n",
        "print(f\"Starting job {load_job.job_id} to load data into {table_id}\")\n",
        "\n",
        "load_job.result()  # Waits for the job to complete.\n",
        "\n",
        "destination_table = bq_client.get_table(table_id)\n",
        "print(f\"Load job finished. Loaded {destination_table.num_rows} rows.\")"
      ],
      "id": "IB0dyMocY5kF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT-5fpH-Y5kF"
      },
      "source": [
        "## 6. (Optional) Cleanup\n",
        "\n",
        "Run the following cell to delete the resources created in this notebook."
      ],
      "id": "IT-5fpH-Y5kF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "title": "Cleanup Resources",
        "id": "KDNh7QagY5kF"
      },
      "outputs": [],
      "source": [
        "# Set to True to delete the created resources\n",
        "delete_resources = False #@param {type:\"boolean\"}\n",
        "\n",
        "if delete_resources:\n",
        "    # Delete BigQuery table\n",
        "    print(f\"Deleting BigQuery table: {table_id}\")\n",
        "    bq_client.delete_table(table_id, not_found_ok=True)\n",
        "    print(\"Table deleted.\")\n",
        "\n",
        "    # Delete GCS file\n",
        "    print(f\"Deleting GCS file: {GENERATED_JSONL_GCS_PATH}\")\n",
        "    try:\n",
        "        storage_client = storage.Client(project=PROJECT_ID)\n",
        "        bucket = storage_client.bucket(GCS_BUCKET_NAME)\n",
        "        blob = bucket.blob(destination_blob_name)\n",
        "        blob.delete()\n",
        "        print(\"GCS file deleted.\")\n",
        "    except google.api_core.exceptions.NotFound:\n",
        "        print(\"GCS file not found, skipping deletion.\")\n",
        "else:\n",
        "    print(\"Cleanup skipped. Set 'delete_resources' to True to delete created resources.\")"
      ],
      "id": "KDNh7QagY5kF"
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "niS180NWVB5-"
      },
      "id": "niS180NWVB5-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#\n",
        "# CELL 1: SETUP AND CONFIGURATION (FOR VERTEX AI)\n",
        "#\n",
        "# This cell contains all the variables you need to change.\n",
        "# Fill these out with your specific details before running the script.\n",
        "#\n",
        "import os\n",
        "from google.cloud import secretmanager\n",
        "from datetime import datetime\n",
        "\n",
        "# --- ⚠️ ACTION REQUIRED: Replace these placeholder values ---\n",
        "GCP_PROJECT_ID = \"partarch-ecommerce-demo\"\n",
        "SECRET_ID = \"github-token\"  # The name you gave the secret in Secret Manager\n",
        "GITHUB_USERNAME = \"cloud-jake\"\n",
        "GITHUB_EMAIL = \"jake.holmquist@gmail.com\"\n",
        "REPO_NAME = \"colab-catalog-generation\"\n",
        "# --- End of required changes ---\n",
        "\n",
        "# --- 1. Securely retrieve the GitHub token from GCP Secret Manager ---\n",
        "print(\"🔑 Accessing GitHub token from GCP Secret Manager...\")\n",
        "\n",
        "def access_secret_version(project_id, secret_id, version_id=\"latest\"):\n",
        "    \"\"\"\n",
        "    Access the payload for the given secret version and return it.\n",
        "    \"\"\"\n",
        "    client = secretmanager.SecretManagerServiceClient()\n",
        "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
        "    response = client.access_secret_version(request={\"name\": name})\n",
        "    return response.payload.data.decode(\"UTF-8\")\n",
        "\n",
        "try:\n",
        "    GIT_TOKEN = access_secret_version(GCP_PROJECT_ID, SECRET_ID)\n",
        "    print(\"   ✅ Token accessed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"   ❌ Error accessing secret: {e}\")\n",
        "    raise Exception(\"Could not access secret. Check permissions and that the secret exists.\")\n",
        "\n",
        "# --- 2. Define repository URL and local path ---\n",
        "repo_url = f\"https://{GITHUB_USERNAME}:{GIT_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "repo_path = f\"/content/{REPO_NAME}\"\n",
        "\n",
        "# --- 3. Clone the repository or pull latest changes ---\n",
        "print(f\"\\n📂 Checking for repository at '{repo_path}'...\")\n",
        "if os.path.exists(repo_path):\n",
        "    print(\"   Repository already exists. Pulling latest changes...\")\n",
        "    %cd {repo_path}\n",
        "    !git pull\n",
        "    %cd /content\n",
        "    print(\"   ✅ Pull complete.\")\n",
        "else:\n",
        "    print(\"   Repository not found. Cloning from GitHub...\")\n",
        "    !git clone {repo_url}\n",
        "    print(\"   ✅ Clone complete.\")\n",
        "\n",
        "# --- 4. Configure Git for commits ---\n",
        "print(\"\\n👤 Configuring Git user...\")\n",
        "%cd {repo_path}\n",
        "!git config user.name \"{GITHUB_USERNAME}\"\n",
        "!git config user.email \"{GITHUB_EMAIL}\"\n",
        "print(\"   ✅ Git user configured.\")\n",
        "\n",
        "# --- 5. Copy all files from /content into the repository ---\n",
        "print(\"\\n📋 Copying all files from /content directory...\")\n",
        "all_content_items = os.listdir('/content')\n",
        "items_to_copy = [item for item in all_content_items if item not in [REPO_NAME, 'sample_data']]\n",
        "\n",
        "if not items_to_copy:\n",
        "    print(\"   No new files to copy.\")\n",
        "else:\n",
        "    for item in items_to_copy:\n",
        "        source_path = f\"/content/{item}\"\n",
        "        print(f\"   - Copying '{source_path}'...\")\n",
        "        !cp -r \"{source_path}\" .\n",
        "    print(\"   ✅ All files copied.\")\n",
        "\n",
        "# --- 6. Add, commit, and push changes ---\n",
        "print(\"\\n🚀 Staging, committing, and pushing changes to GitHub...\")\n",
        "!git add .\n",
        "commit_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "commit_message = f\"Update from Colab Enterprise at {commit_time}\"\n",
        "!git commit -m \"{commit_message}\"\n",
        "!git push -u origin main\n",
        "\n",
        "print(\"\\n🎉 Success! All changes have been pushed to your GitHub repository.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "pNwWMKI_N1al"
      },
      "id": "pNwWMKI_N1al",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6fb6a4d"
      },
      "source": [],
      "id": "d6fb6a4d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "590cf7d5"
      },
      "source": [],
      "id": "590cf7d5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "vertex_ai_catalog_generation.ipynb"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}